{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ffn_multi_class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMl0t0QRt0Dd4pG3uNuTt0G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visheshagrawal/Deep_Learning_Sequences/blob/master/bi_lstm_multi_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9C9nKlvaAMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets,transforms\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchsample.modules import ModuleTrainer\n",
        "\n",
        "# tr=datasets.MNIST(\"\",train=True,download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
        "train = np.load('reduced_train.npz')\n",
        "x_train = train['X_train']\n",
        "# x_train = torch.from_numpy(x_train)\n",
        "y_train = train['y_train']\n",
        "# y_train = torch.from_numpy(y_train)\n",
        "# mask_train = train['mask_train']\n",
        "#print(x_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6msgwMeutWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4b5cef1-bbdf-4c97-b82d-582534ac46ba"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6 6 6 ... 4 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWnoT-ghc7br",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d02b9d56-c22b-4435-ef0f-56df2eac9a0b"
      },
      "source": [
        "z_train = torch.utils.data.TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
        "print(z_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor(6.))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtVruufYckBl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOXWmSukfkWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c6df362-ece5-4bd6-afb9-2e6afb26f33b"
      },
      "source": [
        "val = np.load('reduced_val.npz')\n",
        "x_val = val['X_val']\n",
        "#x_val = torch.from_numpy(x_val)\n",
        "y_val = val['y_val']\n",
        "#y_val = torch.from_numpy(y_val)\n",
        "mask_val = val['mask_val']\n",
        "print(x_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(635, 400, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W_liwD0sj4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "a504f04a-86f4-4ded-a855-313bf32de559"
      },
      "source": [
        "z_test = torch.utils.data.TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n",
        "print(z_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.2400, 0.0000, 0.2700,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]), tensor(6.))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V-dd64dqEM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader= torch.utils.data.DataLoader(z_train,64,True)\n",
        "#print(z_train)\n",
        "#print(train_loader)\n",
        "test_loader= torch.utils.data.DataLoader(z_test,64,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGRRbHpjg67S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "seq_len = 400\n",
        "n_feat = 20\n",
        "n_hid = 30\n",
        "n_class = 10\n",
        "lr = 0.0025\n",
        "n_filt = 10\n",
        "drop_prob = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HvmndzzN2Hp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "aed8b0b1-875b-495d-8fc0-df4982c1d050"
      },
      "source": [
        "for batch_idx,(data,target) in enumerate(train_loader):\n",
        "  print(len(train_loader))\n",
        "  data=data.view(400,-1,20)\n",
        "  print(data.size())\n",
        "  print(target.size())\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38\n",
            "torch.Size([400, 64, 20])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPnertrZ25Ky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "63f47416-264d-471e-de5d-4f7c01f69b32"
      },
      "source": [
        "for data,target in test_loader:\n",
        "  print(data.view(64,-1).size())\n",
        "  print(target.size())\n",
        "  print(data.shape[0])\n",
        "  print(len(test_loader))\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 8000])\n",
            "torch.Size([64])\n",
            "64\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3mWxl8cHe9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Sequential(nn.Linear(12000,60),nn.ReLU(),nn.Linear(60,output_dim),nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(1), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(1), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # 28 time steps8\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        #print(out.shape)\n",
        "        # Index hidden state of last time step\n",
        "        # out.size() --> 100, 28, 100\n",
        "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
        "        #out = out.view(-1,12000)\n",
        "        out=torch.reshape(out,(-1,12000))\n",
        "        #print(out.shape)\n",
        "        out = self.fc(out)\n",
        "        # out.size() --> 100, 10\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBBVbg2JJuO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(BLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim,bidirectional=True)\n",
        "        #self.fc = nn.Sequential(nn.Linear(hidden_dim*2,100),nn.ReLU(),nn.Linear(100,output_dim),nn.LogSoftmax(dim=1))\n",
        "        #self.fc = nn.Linear(hidden_dim*2,output_dim)\n",
        "        #self.fc = nn.Sequential(nn.Linear(hidden_dim*2,output_dim),nn.LogSoftmax(dim=1))\n",
        "        self.fc = nn.Sequential(nn.Linear(48000,2000),nn.ReLU(),nn.Linear(2000,100),nn.ReLU(),nn.Linear(100,output_dim),nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim*2, x.size(1), self.hidden_dim).requires_grad_()\n",
        "        c0 = torch.zeros(self.layer_dim*2, x.size(1), self.hidden_dim).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "        #out = self.fc(out[-1,:,:])\n",
        "        #out=torch.reshape(out,(-1,24000))\n",
        "        out= self.fc(out.view(-1,48000))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwRfiLLCdtcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=BLSTM(20,60,1,10)\n",
        "\n",
        "criterion= nn.NLLLoss()\n",
        "\n",
        "#criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.003)\n",
        "#optimizer2=optim.Adam(fnnmodel.parameters(),lr=0.0025)\n",
        "\n",
        "epochs=10\n",
        "\n",
        "trainer = ModelTrainer(model)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4OZDJWTa6ZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f0a45a4-94a6-49e4-e896-42026d80f86f"
      },
      "source": [
        "#model=nn.Sequential(nn.Linear(8000,n_hid),nn.ReLU(),nn.Linear(n_hid,n_class),nn.LogSoftmax(dim=1))\n",
        "\n",
        "# lstmmodel=nn.LSTM(20,30,1)\n",
        "\n",
        "# fnnmodel=nn.Sequential(nn.Linear(12000,30),nn.ReLU(),nn.Linear(30,n_class),nn.LogSoftmax(dim=1))\n",
        "\n",
        "#model=LSTMModel(20,30,1,10)\n",
        "\n",
        "model=BLSTM(20,60,1,10)\n",
        "\n",
        "criterion= nn.NLLLoss()\n",
        "\n",
        "#criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.003)\n",
        "#optimizer2=optim.Adam(fnnmodel.parameters(),lr=0.0025)\n",
        "\n",
        "epochs=10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss=0\n",
        "  total_loss=0\n",
        "  for batch_idx,(data,target) in enumerate(train_loader):\n",
        "    target=target.type(torch.LongTensor)\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    data=data.view(400,-1,20)\n",
        "    sh=data.shape[1]\n",
        "    #forward\n",
        "    output = model(data)\n",
        "    loss=criterion(output,target)\n",
        "    \n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    #adam step\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    # if batch_idx % 10 == 0:\n",
        "    # #if True:\n",
        "    #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "    #   epoch, batch_idx * data.shape[1], len(train_loader.dataset),\n",
        "    #   100. * batch_idx / len(train_loader), loss.item()))\n",
        "    total_loss+=running_loss\n",
        "    print(running_loss)\n",
        "    print(\"Training epoch: \"+str(epoch)+\" loss for this batch is \"+str(running_loss))\n",
        "    running_loss=0\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data=data.view(-1,8000)\n",
        "    sh1=data.shape[0]\n",
        "    target=target.type(torch.LongTensor)\n",
        "    # h0 = torch.randn(1, sh1, 30)\n",
        "    # c0 = torch.randn(1, sh1, 30)\n",
        "    # hidden=(h0,c0)\n",
        "    data=data.view(400,-1,20)\n",
        "    data, target = Variable(data, volatile=True), Variable(target)\n",
        "    output = model(data)\n",
        "    #output,hid=lstmmodel(data,hidden)\n",
        "    #output=output.view(-1,12000)\n",
        "    #fin_output=fnnmodel(output)\n",
        "    #loss=criterion(fin_output,target)\n",
        "    # sum up batch loss\n",
        "    test_loss += criterion(output,target)\n",
        "    # get the index of the max log-probability\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  #test_loss /= len(test_loader.dataset)\n",
        "  test_loss /= batch_size\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      test_loss, correct, len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))\n",
        "  print(str(total_loss/(batch_idx+1)))\n",
        "  print(\"Epoch finished\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2920620441436768\n",
            "Training epoch: 0 loss for this batch is 2.2920620441436768\n",
            "10.214472770690918\n",
            "Training epoch: 0 loss for this batch is 10.214472770690918\n",
            "10.706966400146484\n",
            "Training epoch: 0 loss for this batch is 10.706966400146484\n",
            "5.4987030029296875\n",
            "Training epoch: 0 loss for this batch is 5.4987030029296875\n",
            "3.5156853199005127\n",
            "Training epoch: 0 loss for this batch is 3.5156853199005127\n",
            "3.1979963779449463\n",
            "Training epoch: 0 loss for this batch is 3.1979963779449463\n",
            "2.4503705501556396\n",
            "Training epoch: 0 loss for this batch is 2.4503705501556396\n",
            "2.2560675144195557\n",
            "Training epoch: 0 loss for this batch is 2.2560675144195557\n",
            "2.206488609313965\n",
            "Training epoch: 0 loss for this batch is 2.206488609313965\n",
            "2.122464895248413\n",
            "Training epoch: 0 loss for this batch is 2.122464895248413\n",
            "2.2142844200134277\n",
            "Training epoch: 0 loss for this batch is 2.2142844200134277\n",
            "2.0483896732330322\n",
            "Training epoch: 0 loss for this batch is 2.0483896732330322\n",
            "2.073277235031128\n",
            "Training epoch: 0 loss for this batch is 2.073277235031128\n",
            "2.002828598022461\n",
            "Training epoch: 0 loss for this batch is 2.002828598022461\n",
            "2.091590642929077\n",
            "Training epoch: 0 loss for this batch is 2.091590642929077\n",
            "2.1077260971069336\n",
            "Training epoch: 0 loss for this batch is 2.1077260971069336\n",
            "1.9457646608352661\n",
            "Training epoch: 0 loss for this batch is 1.9457646608352661\n",
            "2.08866810798645\n",
            "Training epoch: 0 loss for this batch is 2.08866810798645\n",
            "2.056563138961792\n",
            "Training epoch: 0 loss for this batch is 2.056563138961792\n",
            "1.9040671586990356\n",
            "Training epoch: 0 loss for this batch is 1.9040671586990356\n",
            "1.8855353593826294\n",
            "Training epoch: 0 loss for this batch is 1.8855353593826294\n",
            "2.2708520889282227\n",
            "Training epoch: 0 loss for this batch is 2.2708520889282227\n",
            "1.8241808414459229\n",
            "Training epoch: 0 loss for this batch is 1.8241808414459229\n",
            "1.8611716032028198\n",
            "Training epoch: 0 loss for this batch is 1.8611716032028198\n",
            "2.1369400024414062\n",
            "Training epoch: 0 loss for this batch is 2.1369400024414062\n",
            "1.9903275966644287\n",
            "Training epoch: 0 loss for this batch is 1.9903275966644287\n",
            "2.0319252014160156\n",
            "Training epoch: 0 loss for this batch is 2.0319252014160156\n",
            "2.0360898971557617\n",
            "Training epoch: 0 loss for this batch is 2.0360898971557617\n",
            "1.9365017414093018\n",
            "Training epoch: 0 loss for this batch is 1.9365017414093018\n",
            "1.9610763788223267\n",
            "Training epoch: 0 loss for this batch is 1.9610763788223267\n",
            "1.911772608757019\n",
            "Training epoch: 0 loss for this batch is 1.911772608757019\n",
            "1.8800997734069824\n",
            "Training epoch: 0 loss for this batch is 1.8800997734069824\n",
            "2.002060890197754\n",
            "Training epoch: 0 loss for this batch is 2.002060890197754\n",
            "2.062810182571411\n",
            "Training epoch: 0 loss for this batch is 2.062810182571411\n",
            "1.963018774986267\n",
            "Training epoch: 0 loss for this batch is 1.963018774986267\n",
            "2.0289478302001953\n",
            "Training epoch: 0 loss for this batch is 2.0289478302001953\n",
            "1.9766212701797485\n",
            "Training epoch: 0 loss for this batch is 1.9766212701797485\n",
            "1.8720414638519287\n",
            "Training epoch: 0 loss for this batch is 1.8720414638519287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.3021, Accuracy: 187/635 (29%)\n",
            "\n",
            "2.648063440071909\n",
            "Epoch finished\n",
            "1.933566927909851\n",
            "Training epoch: 1 loss for this batch is 1.933566927909851\n",
            "2.0640337467193604\n",
            "Training epoch: 1 loss for this batch is 2.0640337467193604\n",
            "1.9009298086166382\n",
            "Training epoch: 1 loss for this batch is 1.9009298086166382\n",
            "1.8503166437149048\n",
            "Training epoch: 1 loss for this batch is 1.8503166437149048\n",
            "1.9655139446258545\n",
            "Training epoch: 1 loss for this batch is 1.9655139446258545\n",
            "1.915433406829834\n",
            "Training epoch: 1 loss for this batch is 1.915433406829834\n",
            "1.8886629343032837\n",
            "Training epoch: 1 loss for this batch is 1.8886629343032837\n",
            "1.757684588432312\n",
            "Training epoch: 1 loss for this batch is 1.757684588432312\n",
            "1.941969633102417\n",
            "Training epoch: 1 loss for this batch is 1.941969633102417\n",
            "1.7436027526855469\n",
            "Training epoch: 1 loss for this batch is 1.7436027526855469\n",
            "1.9631675481796265\n",
            "Training epoch: 1 loss for this batch is 1.9631675481796265\n",
            "1.7343627214431763\n",
            "Training epoch: 1 loss for this batch is 1.7343627214431763\n",
            "1.941314697265625\n",
            "Training epoch: 1 loss for this batch is 1.941314697265625\n",
            "1.7716925144195557\n",
            "Training epoch: 1 loss for this batch is 1.7716925144195557\n",
            "1.9078724384307861\n",
            "Training epoch: 1 loss for this batch is 1.9078724384307861\n",
            "1.8256301879882812\n",
            "Training epoch: 1 loss for this batch is 1.8256301879882812\n",
            "1.9879047870635986\n",
            "Training epoch: 1 loss for this batch is 1.9879047870635986\n",
            "1.8195973634719849\n",
            "Training epoch: 1 loss for this batch is 1.8195973634719849\n",
            "2.0304648876190186\n",
            "Training epoch: 1 loss for this batch is 2.0304648876190186\n",
            "2.0278987884521484\n",
            "Training epoch: 1 loss for this batch is 2.0278987884521484\n",
            "1.8275537490844727\n",
            "Training epoch: 1 loss for this batch is 1.8275537490844727\n",
            "1.8539388179779053\n",
            "Training epoch: 1 loss for this batch is 1.8539388179779053\n",
            "1.9494614601135254\n",
            "Training epoch: 1 loss for this batch is 1.9494614601135254\n",
            "1.8563220500946045\n",
            "Training epoch: 1 loss for this batch is 1.8563220500946045\n",
            "2.137007236480713\n",
            "Training epoch: 1 loss for this batch is 2.137007236480713\n",
            "1.7930376529693604\n",
            "Training epoch: 1 loss for this batch is 1.7930376529693604\n",
            "1.8689154386520386\n",
            "Training epoch: 1 loss for this batch is 1.8689154386520386\n",
            "1.9126077890396118\n",
            "Training epoch: 1 loss for this batch is 1.9126077890396118\n",
            "1.9777981042861938\n",
            "Training epoch: 1 loss for this batch is 1.9777981042861938\n",
            "2.0110349655151367\n",
            "Training epoch: 1 loss for this batch is 2.0110349655151367\n",
            "1.7795580625534058\n",
            "Training epoch: 1 loss for this batch is 1.7795580625534058\n",
            "2.0598933696746826\n",
            "Training epoch: 1 loss for this batch is 2.0598933696746826\n",
            "1.904937982559204\n",
            "Training epoch: 1 loss for this batch is 1.904937982559204\n",
            "2.000840425491333\n",
            "Training epoch: 1 loss for this batch is 2.000840425491333\n",
            "1.8244273662567139\n",
            "Training epoch: 1 loss for this batch is 1.8244273662567139\n",
            "1.9020969867706299\n",
            "Training epoch: 1 loss for this batch is 1.9020969867706299\n",
            "1.9410009384155273\n",
            "Training epoch: 1 loss for this batch is 1.9410009384155273\n",
            "1.9952318668365479\n",
            "Training epoch: 1 loss for this batch is 1.9952318668365479\n",
            "\n",
            "Test set: Average loss: 0.2944, Accuracy: 196/635 (31%)\n",
            "\n",
            "1.9096653837906687\n",
            "Epoch finished\n",
            "1.859359860420227\n",
            "Training epoch: 2 loss for this batch is 1.859359860420227\n",
            "1.8916500806808472\n",
            "Training epoch: 2 loss for this batch is 1.8916500806808472\n",
            "1.7470141649246216\n",
            "Training epoch: 2 loss for this batch is 1.7470141649246216\n",
            "1.8522313833236694\n",
            "Training epoch: 2 loss for this batch is 1.8522313833236694\n",
            "1.957166075706482\n",
            "Training epoch: 2 loss for this batch is 1.957166075706482\n",
            "1.8593441247940063\n",
            "Training epoch: 2 loss for this batch is 1.8593441247940063\n",
            "1.7582669258117676\n",
            "Training epoch: 2 loss for this batch is 1.7582669258117676\n",
            "1.877415657043457\n",
            "Training epoch: 2 loss for this batch is 1.877415657043457\n",
            "1.9558888673782349\n",
            "Training epoch: 2 loss for this batch is 1.9558888673782349\n",
            "1.8068180084228516\n",
            "Training epoch: 2 loss for this batch is 1.8068180084228516\n",
            "1.7172753810882568\n",
            "Training epoch: 2 loss for this batch is 1.7172753810882568\n",
            "1.7368415594100952\n",
            "Training epoch: 2 loss for this batch is 1.7368415594100952\n",
            "1.7710957527160645\n",
            "Training epoch: 2 loss for this batch is 1.7710957527160645\n",
            "1.8760709762573242\n",
            "Training epoch: 2 loss for this batch is 1.8760709762573242\n",
            "1.7503564357757568\n",
            "Training epoch: 2 loss for this batch is 1.7503564357757568\n",
            "1.6645992994308472\n",
            "Training epoch: 2 loss for this batch is 1.6645992994308472\n",
            "1.800328016281128\n",
            "Training epoch: 2 loss for this batch is 1.800328016281128\n",
            "1.7407255172729492\n",
            "Training epoch: 2 loss for this batch is 1.7407255172729492\n",
            "1.830111026763916\n",
            "Training epoch: 2 loss for this batch is 1.830111026763916\n",
            "1.6929951906204224\n",
            "Training epoch: 2 loss for this batch is 1.6929951906204224\n",
            "1.8828102350234985\n",
            "Training epoch: 2 loss for this batch is 1.8828102350234985\n",
            "1.5654178857803345\n",
            "Training epoch: 2 loss for this batch is 1.5654178857803345\n",
            "1.7458049058914185\n",
            "Training epoch: 2 loss for this batch is 1.7458049058914185\n",
            "1.7913568019866943\n",
            "Training epoch: 2 loss for this batch is 1.7913568019866943\n",
            "1.8435908555984497\n",
            "Training epoch: 2 loss for this batch is 1.8435908555984497\n",
            "1.6594903469085693\n",
            "Training epoch: 2 loss for this batch is 1.6594903469085693\n",
            "1.8931578397750854\n",
            "Training epoch: 2 loss for this batch is 1.8931578397750854\n",
            "1.7339917421340942\n",
            "Training epoch: 2 loss for this batch is 1.7339917421340942\n",
            "1.6509658098220825\n",
            "Training epoch: 2 loss for this batch is 1.6509658098220825\n",
            "1.4893462657928467\n",
            "Training epoch: 2 loss for this batch is 1.4893462657928467\n",
            "1.6251286268234253\n",
            "Training epoch: 2 loss for this batch is 1.6251286268234253\n",
            "1.8412774801254272\n",
            "Training epoch: 2 loss for this batch is 1.8412774801254272\n",
            "1.556570053100586\n",
            "Training epoch: 2 loss for this batch is 1.556570053100586\n",
            "1.5674190521240234\n",
            "Training epoch: 2 loss for this batch is 1.5674190521240234\n",
            "1.4563281536102295\n",
            "Training epoch: 2 loss for this batch is 1.4563281536102295\n",
            "1.5930113792419434\n",
            "Training epoch: 2 loss for this batch is 1.5930113792419434\n",
            "1.613439917564392\n",
            "Training epoch: 2 loss for this batch is 1.613439917564392\n",
            "1.5921369791030884\n",
            "Training epoch: 2 loss for this batch is 1.5921369791030884\n",
            "\n",
            "Test set: Average loss: 0.2460, Accuracy: 277/635 (44%)\n",
            "\n",
            "1.7433368061718189\n",
            "Epoch finished\n",
            "1.5783097743988037\n",
            "Training epoch: 3 loss for this batch is 1.5783097743988037\n",
            "1.5505517721176147\n",
            "Training epoch: 3 loss for this batch is 1.5505517721176147\n",
            "1.4226326942443848\n",
            "Training epoch: 3 loss for this batch is 1.4226326942443848\n",
            "1.7551724910736084\n",
            "Training epoch: 3 loss for this batch is 1.7551724910736084\n",
            "1.1550203561782837\n",
            "Training epoch: 3 loss for this batch is 1.1550203561782837\n",
            "1.4559358358383179\n",
            "Training epoch: 3 loss for this batch is 1.4559358358383179\n",
            "1.4005389213562012\n",
            "Training epoch: 3 loss for this batch is 1.4005389213562012\n",
            "1.4980661869049072\n",
            "Training epoch: 3 loss for this batch is 1.4980661869049072\n",
            "1.2376676797866821\n",
            "Training epoch: 3 loss for this batch is 1.2376676797866821\n",
            "1.155539631843567\n",
            "Training epoch: 3 loss for this batch is 1.155539631843567\n",
            "1.2065801620483398\n",
            "Training epoch: 3 loss for this batch is 1.2065801620483398\n",
            "1.5677090883255005\n",
            "Training epoch: 3 loss for this batch is 1.5677090883255005\n",
            "1.199430227279663\n",
            "Training epoch: 3 loss for this batch is 1.199430227279663\n",
            "1.3776582479476929\n",
            "Training epoch: 3 loss for this batch is 1.3776582479476929\n",
            "1.3582448959350586\n",
            "Training epoch: 3 loss for this batch is 1.3582448959350586\n",
            "1.280655860900879\n",
            "Training epoch: 3 loss for this batch is 1.280655860900879\n",
            "1.4178770780563354\n",
            "Training epoch: 3 loss for this batch is 1.4178770780563354\n",
            "1.062666654586792\n",
            "Training epoch: 3 loss for this batch is 1.062666654586792\n",
            "1.1718913316726685\n",
            "Training epoch: 3 loss for this batch is 1.1718913316726685\n",
            "1.1740397214889526\n",
            "Training epoch: 3 loss for this batch is 1.1740397214889526\n",
            "1.0763200521469116\n",
            "Training epoch: 3 loss for this batch is 1.0763200521469116\n",
            "0.9823940992355347\n",
            "Training epoch: 3 loss for this batch is 0.9823940992355347\n",
            "0.9535970687866211\n",
            "Training epoch: 3 loss for this batch is 0.9535970687866211\n",
            "1.0631035566329956\n",
            "Training epoch: 3 loss for this batch is 1.0631035566329956\n",
            "1.2513458728790283\n",
            "Training epoch: 3 loss for this batch is 1.2513458728790283\n",
            "1.1713889837265015\n",
            "Training epoch: 3 loss for this batch is 1.1713889837265015\n",
            "1.1975408792495728\n",
            "Training epoch: 3 loss for this batch is 1.1975408792495728\n",
            "1.0374140739440918\n",
            "Training epoch: 3 loss for this batch is 1.0374140739440918\n",
            "0.9248930811882019\n",
            "Training epoch: 3 loss for this batch is 0.9248930811882019\n",
            "1.0327417850494385\n",
            "Training epoch: 3 loss for this batch is 1.0327417850494385\n",
            "0.8319028615951538\n",
            "Training epoch: 3 loss for this batch is 0.8319028615951538\n",
            "0.732867956161499\n",
            "Training epoch: 3 loss for this batch is 0.732867956161499\n",
            "0.9374276399612427\n",
            "Training epoch: 3 loss for this batch is 0.9374276399612427\n",
            "0.9816781282424927\n",
            "Training epoch: 3 loss for this batch is 0.9816781282424927\n",
            "0.8811991214752197\n",
            "Training epoch: 3 loss for this batch is 0.8811991214752197\n",
            "1.1247138977050781\n",
            "Training epoch: 3 loss for this batch is 1.1247138977050781\n",
            "0.8438321352005005\n",
            "Training epoch: 3 loss for this batch is 0.8438321352005005\n",
            "0.7993501424789429\n",
            "Training epoch: 3 loss for this batch is 0.7993501424789429\n",
            "\n",
            "Test set: Average loss: 0.1718, Accuracy: 404/635 (64%)\n",
            "\n",
            "1.180260524937981\n",
            "Epoch finished\n",
            "0.607428789138794\n",
            "Training epoch: 4 loss for this batch is 0.607428789138794\n",
            "0.7648682594299316\n",
            "Training epoch: 4 loss for this batch is 0.7648682594299316\n",
            "0.794823408126831\n",
            "Training epoch: 4 loss for this batch is 0.794823408126831\n",
            "0.812923789024353\n",
            "Training epoch: 4 loss for this batch is 0.812923789024353\n",
            "0.8358089923858643\n",
            "Training epoch: 4 loss for this batch is 0.8358089923858643\n",
            "0.8946743011474609\n",
            "Training epoch: 4 loss for this batch is 0.8946743011474609\n",
            "0.7085950374603271\n",
            "Training epoch: 4 loss for this batch is 0.7085950374603271\n",
            "0.49229568243026733\n",
            "Training epoch: 4 loss for this batch is 0.49229568243026733\n",
            "0.7887279987335205\n",
            "Training epoch: 4 loss for this batch is 0.7887279987335205\n",
            "0.7856389880180359\n",
            "Training epoch: 4 loss for this batch is 0.7856389880180359\n",
            "0.8389049172401428\n",
            "Training epoch: 4 loss for this batch is 0.8389049172401428\n",
            "0.5390825271606445\n",
            "Training epoch: 4 loss for this batch is 0.5390825271606445\n",
            "0.8773711323738098\n",
            "Training epoch: 4 loss for this batch is 0.8773711323738098\n",
            "0.7643231153488159\n",
            "Training epoch: 4 loss for this batch is 0.7643231153488159\n",
            "0.8569789528846741\n",
            "Training epoch: 4 loss for this batch is 0.8569789528846741\n",
            "0.5133054256439209\n",
            "Training epoch: 4 loss for this batch is 0.5133054256439209\n",
            "0.8223996162414551\n",
            "Training epoch: 4 loss for this batch is 0.8223996162414551\n",
            "0.5521803498268127\n",
            "Training epoch: 4 loss for this batch is 0.5521803498268127\n",
            "0.6556901931762695\n",
            "Training epoch: 4 loss for this batch is 0.6556901931762695\n",
            "0.3355933129787445\n",
            "Training epoch: 4 loss for this batch is 0.3355933129787445\n",
            "0.7728399038314819\n",
            "Training epoch: 4 loss for this batch is 0.7728399038314819\n",
            "0.71595299243927\n",
            "Training epoch: 4 loss for this batch is 0.71595299243927\n",
            "0.5066456198692322\n",
            "Training epoch: 4 loss for this batch is 0.5066456198692322\n",
            "0.6655727028846741\n",
            "Training epoch: 4 loss for this batch is 0.6655727028846741\n",
            "0.5725717544555664\n",
            "Training epoch: 4 loss for this batch is 0.5725717544555664\n",
            "0.6010157465934753\n",
            "Training epoch: 4 loss for this batch is 0.6010157465934753\n",
            "0.47442948818206787\n",
            "Training epoch: 4 loss for this batch is 0.47442948818206787\n",
            "0.3649357855319977\n",
            "Training epoch: 4 loss for this batch is 0.3649357855319977\n",
            "0.5678084492683411\n",
            "Training epoch: 4 loss for this batch is 0.5678084492683411\n",
            "0.5371257066726685\n",
            "Training epoch: 4 loss for this batch is 0.5371257066726685\n",
            "0.47467076778411865\n",
            "Training epoch: 4 loss for this batch is 0.47467076778411865\n",
            "0.49206751585006714\n",
            "Training epoch: 4 loss for this batch is 0.49206751585006714\n",
            "0.5999224185943604\n",
            "Training epoch: 4 loss for this batch is 0.5999224185943604\n",
            "0.6152331233024597\n",
            "Training epoch: 4 loss for this batch is 0.6152331233024597\n",
            "0.35102933645248413\n",
            "Training epoch: 4 loss for this batch is 0.35102933645248413\n",
            "0.8595280647277832\n",
            "Training epoch: 4 loss for this batch is 0.8595280647277832\n",
            "0.5278269648551941\n",
            "Training epoch: 4 loss for this batch is 0.5278269648551941\n",
            "0.5753604173660278\n",
            "Training epoch: 4 loss for this batch is 0.5753604173660278\n",
            "\n",
            "Test set: Average loss: 0.1434, Accuracy: 467/635 (74%)\n",
            "\n",
            "0.6451618828271565\n",
            "Epoch finished\n",
            "0.425686776638031\n",
            "Training epoch: 5 loss for this batch is 0.425686776638031\n",
            "0.35339096188545227\n",
            "Training epoch: 5 loss for this batch is 0.35339096188545227\n",
            "0.3155382573604584\n",
            "Training epoch: 5 loss for this batch is 0.3155382573604584\n",
            "0.25522738695144653\n",
            "Training epoch: 5 loss for this batch is 0.25522738695144653\n",
            "0.4812781810760498\n",
            "Training epoch: 5 loss for this batch is 0.4812781810760498\n",
            "0.21112790703773499\n",
            "Training epoch: 5 loss for this batch is 0.21112790703773499\n",
            "0.2991168797016144\n",
            "Training epoch: 5 loss for this batch is 0.2991168797016144\n",
            "0.36709821224212646\n",
            "Training epoch: 5 loss for this batch is 0.36709821224212646\n",
            "0.2629726529121399\n",
            "Training epoch: 5 loss for this batch is 0.2629726529121399\n",
            "0.31919384002685547\n",
            "Training epoch: 5 loss for this batch is 0.31919384002685547\n",
            "0.21840612590312958\n",
            "Training epoch: 5 loss for this batch is 0.21840612590312958\n",
            "0.3543189465999603\n",
            "Training epoch: 5 loss for this batch is 0.3543189465999603\n",
            "0.33651575446128845\n",
            "Training epoch: 5 loss for this batch is 0.33651575446128845\n",
            "0.34543749690055847\n",
            "Training epoch: 5 loss for this batch is 0.34543749690055847\n",
            "0.31747955083847046\n",
            "Training epoch: 5 loss for this batch is 0.31747955083847046\n",
            "0.2297571748495102\n",
            "Training epoch: 5 loss for this batch is 0.2297571748495102\n",
            "0.3271304965019226\n",
            "Training epoch: 5 loss for this batch is 0.3271304965019226\n",
            "0.15389443933963776\n",
            "Training epoch: 5 loss for this batch is 0.15389443933963776\n",
            "0.34438446164131165\n",
            "Training epoch: 5 loss for this batch is 0.34438446164131165\n",
            "0.3169316351413727\n",
            "Training epoch: 5 loss for this batch is 0.3169316351413727\n",
            "0.39967700839042664\n",
            "Training epoch: 5 loss for this batch is 0.39967700839042664\n",
            "0.33322855830192566\n",
            "Training epoch: 5 loss for this batch is 0.33322855830192566\n",
            "0.2558892071247101\n",
            "Training epoch: 5 loss for this batch is 0.2558892071247101\n",
            "0.2433728724718094\n",
            "Training epoch: 5 loss for this batch is 0.2433728724718094\n",
            "0.4846733510494232\n",
            "Training epoch: 5 loss for this batch is 0.4846733510494232\n",
            "0.38972485065460205\n",
            "Training epoch: 5 loss for this batch is 0.38972485065460205\n",
            "0.25824519991874695\n",
            "Training epoch: 5 loss for this batch is 0.25824519991874695\n",
            "0.2801400423049927\n",
            "Training epoch: 5 loss for this batch is 0.2801400423049927\n",
            "0.16630777716636658\n",
            "Training epoch: 5 loss for this batch is 0.16630777716636658\n",
            "0.19818229973316193\n",
            "Training epoch: 5 loss for this batch is 0.19818229973316193\n",
            "0.3715971112251282\n",
            "Training epoch: 5 loss for this batch is 0.3715971112251282\n",
            "0.2723753750324249\n",
            "Training epoch: 5 loss for this batch is 0.2723753750324249\n",
            "0.19820262491703033\n",
            "Training epoch: 5 loss for this batch is 0.19820262491703033\n",
            "0.401140034198761\n",
            "Training epoch: 5 loss for this batch is 0.401140034198761\n",
            "0.259158730506897\n",
            "Training epoch: 5 loss for this batch is 0.259158730506897\n",
            "0.2516094744205475\n",
            "Training epoch: 5 loss for this batch is 0.2516094744205475\n",
            "0.3321479558944702\n",
            "Training epoch: 5 loss for this batch is 0.3321479558944702\n",
            "0.10771876573562622\n",
            "Training epoch: 5 loss for this batch is 0.10771876573562622\n",
            "\n",
            "Test set: Average loss: 0.1399, Accuracy: 464/635 (73%)\n",
            "\n",
            "0.3010073257120032\n",
            "Epoch finished\n",
            "0.04784760624170303\n",
            "Training epoch: 6 loss for this batch is 0.04784760624170303\n",
            "0.10041776299476624\n",
            "Training epoch: 6 loss for this batch is 0.10041776299476624\n",
            "0.2751500904560089\n",
            "Training epoch: 6 loss for this batch is 0.2751500904560089\n",
            "0.2681634724140167\n",
            "Training epoch: 6 loss for this batch is 0.2681634724140167\n",
            "0.09641586989164352\n",
            "Training epoch: 6 loss for this batch is 0.09641586989164352\n",
            "0.12085405737161636\n",
            "Training epoch: 6 loss for this batch is 0.12085405737161636\n",
            "0.17381596565246582\n",
            "Training epoch: 6 loss for this batch is 0.17381596565246582\n",
            "0.10625749081373215\n",
            "Training epoch: 6 loss for this batch is 0.10625749081373215\n",
            "0.1303997039794922\n",
            "Training epoch: 6 loss for this batch is 0.1303997039794922\n",
            "0.15552420914173126\n",
            "Training epoch: 6 loss for this batch is 0.15552420914173126\n",
            "0.10471145808696747\n",
            "Training epoch: 6 loss for this batch is 0.10471145808696747\n",
            "0.1151554211974144\n",
            "Training epoch: 6 loss for this batch is 0.1151554211974144\n",
            "0.07850275188684464\n",
            "Training epoch: 6 loss for this batch is 0.07850275188684464\n",
            "0.10038957744836807\n",
            "Training epoch: 6 loss for this batch is 0.10038957744836807\n",
            "0.1045089140534401\n",
            "Training epoch: 6 loss for this batch is 0.1045089140534401\n",
            "0.13593684136867523\n",
            "Training epoch: 6 loss for this batch is 0.13593684136867523\n",
            "0.0930909812450409\n",
            "Training epoch: 6 loss for this batch is 0.0930909812450409\n",
            "0.14900600910186768\n",
            "Training epoch: 6 loss for this batch is 0.14900600910186768\n",
            "0.27044016122817993\n",
            "Training epoch: 6 loss for this batch is 0.27044016122817993\n",
            "0.03840109333395958\n",
            "Training epoch: 6 loss for this batch is 0.03840109333395958\n",
            "0.12178642302751541\n",
            "Training epoch: 6 loss for this batch is 0.12178642302751541\n",
            "0.07466286420822144\n",
            "Training epoch: 6 loss for this batch is 0.07466286420822144\n",
            "0.140147402882576\n",
            "Training epoch: 6 loss for this batch is 0.140147402882576\n",
            "0.14430995285511017\n",
            "Training epoch: 6 loss for this batch is 0.14430995285511017\n",
            "0.1008836105465889\n",
            "Training epoch: 6 loss for this batch is 0.1008836105465889\n",
            "0.09721340984106064\n",
            "Training epoch: 6 loss for this batch is 0.09721340984106064\n",
            "0.06031640246510506\n",
            "Training epoch: 6 loss for this batch is 0.06031640246510506\n",
            "0.04543931409716606\n",
            "Training epoch: 6 loss for this batch is 0.04543931409716606\n",
            "0.11457647383213043\n",
            "Training epoch: 6 loss for this batch is 0.11457647383213043\n",
            "0.13015581667423248\n",
            "Training epoch: 6 loss for this batch is 0.13015581667423248\n",
            "0.056033261120319366\n",
            "Training epoch: 6 loss for this batch is 0.056033261120319366\n",
            "0.05729134753346443\n",
            "Training epoch: 6 loss for this batch is 0.05729134753346443\n",
            "0.11854275315999985\n",
            "Training epoch: 6 loss for this batch is 0.11854275315999985\n",
            "0.10829942673444748\n",
            "Training epoch: 6 loss for this batch is 0.10829942673444748\n",
            "0.06566090881824493\n",
            "Training epoch: 6 loss for this batch is 0.06566090881824493\n",
            "0.11015961319208145\n",
            "Training epoch: 6 loss for this batch is 0.11015961319208145\n",
            "0.056164659559726715\n",
            "Training epoch: 6 loss for this batch is 0.056164659559726715\n",
            "0.12337778508663177\n",
            "Training epoch: 6 loss for this batch is 0.12337778508663177\n",
            "\n",
            "Test set: Average loss: 0.1524, Accuracy: 491/635 (77%)\n",
            "\n",
            "0.11552660167217255\n",
            "Epoch finished\n",
            "0.047850508242845535\n",
            "Training epoch: 7 loss for this batch is 0.047850508242845535\n",
            "0.015814444050192833\n",
            "Training epoch: 7 loss for this batch is 0.015814444050192833\n",
            "0.029482688754796982\n",
            "Training epoch: 7 loss for this batch is 0.029482688754796982\n",
            "0.027561930939555168\n",
            "Training epoch: 7 loss for this batch is 0.027561930939555168\n",
            "0.07799790799617767\n",
            "Training epoch: 7 loss for this batch is 0.07799790799617767\n",
            "0.038796987384557724\n",
            "Training epoch: 7 loss for this batch is 0.038796987384557724\n",
            "0.00980657059699297\n",
            "Training epoch: 7 loss for this batch is 0.00980657059699297\n",
            "0.038211312144994736\n",
            "Training epoch: 7 loss for this batch is 0.038211312144994736\n",
            "0.023531045764684677\n",
            "Training epoch: 7 loss for this batch is 0.023531045764684677\n",
            "0.023598896339535713\n",
            "Training epoch: 7 loss for this batch is 0.023598896339535713\n",
            "0.03224336355924606\n",
            "Training epoch: 7 loss for this batch is 0.03224336355924606\n",
            "0.020283488556742668\n",
            "Training epoch: 7 loss for this batch is 0.020283488556742668\n",
            "0.06397940963506699\n",
            "Training epoch: 7 loss for this batch is 0.06397940963506699\n",
            "0.030944017693400383\n",
            "Training epoch: 7 loss for this batch is 0.030944017693400383\n",
            "0.10101228952407837\n",
            "Training epoch: 7 loss for this batch is 0.10101228952407837\n",
            "0.010751171968877316\n",
            "Training epoch: 7 loss for this batch is 0.010751171968877316\n",
            "0.03704965114593506\n",
            "Training epoch: 7 loss for this batch is 0.03704965114593506\n",
            "0.04645969718694687\n",
            "Training epoch: 7 loss for this batch is 0.04645969718694687\n",
            "0.046035561710596085\n",
            "Training epoch: 7 loss for this batch is 0.046035561710596085\n",
            "0.13188976049423218\n",
            "Training epoch: 7 loss for this batch is 0.13188976049423218\n",
            "0.015428191982209682\n",
            "Training epoch: 7 loss for this batch is 0.015428191982209682\n",
            "0.14740678668022156\n",
            "Training epoch: 7 loss for this batch is 0.14740678668022156\n",
            "0.014381439425051212\n",
            "Training epoch: 7 loss for this batch is 0.014381439425051212\n",
            "0.018234634771943092\n",
            "Training epoch: 7 loss for this batch is 0.018234634771943092\n",
            "0.021786827594041824\n",
            "Training epoch: 7 loss for this batch is 0.021786827594041824\n",
            "0.05378599464893341\n",
            "Training epoch: 7 loss for this batch is 0.05378599464893341\n",
            "0.021744919940829277\n",
            "Training epoch: 7 loss for this batch is 0.021744919940829277\n",
            "0.015663012862205505\n",
            "Training epoch: 7 loss for this batch is 0.015663012862205505\n",
            "0.02089884504675865\n",
            "Training epoch: 7 loss for this batch is 0.02089884504675865\n",
            "0.009355318732559681\n",
            "Training epoch: 7 loss for this batch is 0.009355318732559681\n",
            "0.013927406631410122\n",
            "Training epoch: 7 loss for this batch is 0.013927406631410122\n",
            "0.02445470727980137\n",
            "Training epoch: 7 loss for this batch is 0.02445470727980137\n",
            "0.046596430242061615\n",
            "Training epoch: 7 loss for this batch is 0.046596430242061615\n",
            "0.030478570610284805\n",
            "Training epoch: 7 loss for this batch is 0.030478570610284805\n",
            "0.0063616433180868626\n",
            "Training epoch: 7 loss for this batch is 0.0063616433180868626\n",
            "0.0033238008618354797\n",
            "Training epoch: 7 loss for this batch is 0.0033238008618354797\n",
            "0.021644536405801773\n",
            "Training epoch: 7 loss for this batch is 0.021644536405801773\n",
            "0.05032540485262871\n",
            "Training epoch: 7 loss for this batch is 0.05032540485262871\n",
            "\n",
            "Test set: Average loss: 0.1605, Accuracy: 486/635 (77%)\n",
            "\n",
            "0.03655524146252949\n",
            "Epoch finished\n",
            "0.006127283908426762\n",
            "Training epoch: 8 loss for this batch is 0.006127283908426762\n",
            "0.005883845500648022\n",
            "Training epoch: 8 loss for this batch is 0.005883845500648022\n",
            "0.0059782275930047035\n",
            "Training epoch: 8 loss for this batch is 0.0059782275930047035\n",
            "0.009083719924092293\n",
            "Training epoch: 8 loss for this batch is 0.009083719924092293\n",
            "0.007411351427435875\n",
            "Training epoch: 8 loss for this batch is 0.007411351427435875\n",
            "0.005211292766034603\n",
            "Training epoch: 8 loss for this batch is 0.005211292766034603\n",
            "0.0075667425990104675\n",
            "Training epoch: 8 loss for this batch is 0.0075667425990104675\n",
            "0.015520281158387661\n",
            "Training epoch: 8 loss for this batch is 0.015520281158387661\n",
            "0.00809547770768404\n",
            "Training epoch: 8 loss for this batch is 0.00809547770768404\n",
            "0.010327482596039772\n",
            "Training epoch: 8 loss for this batch is 0.010327482596039772\n",
            "0.014204983599483967\n",
            "Training epoch: 8 loss for this batch is 0.014204983599483967\n",
            "0.0089210644364357\n",
            "Training epoch: 8 loss for this batch is 0.0089210644364357\n",
            "0.005024707410484552\n",
            "Training epoch: 8 loss for this batch is 0.005024707410484552\n",
            "0.0050102416425943375\n",
            "Training epoch: 8 loss for this batch is 0.0050102416425943375\n",
            "0.00526399863883853\n",
            "Training epoch: 8 loss for this batch is 0.00526399863883853\n",
            "0.006210933905094862\n",
            "Training epoch: 8 loss for this batch is 0.006210933905094862\n",
            "0.005766683258116245\n",
            "Training epoch: 8 loss for this batch is 0.005766683258116245\n",
            "0.11699455231428146\n",
            "Training epoch: 8 loss for this batch is 0.11699455231428146\n",
            "0.004706844687461853\n",
            "Training epoch: 8 loss for this batch is 0.004706844687461853\n",
            "0.006680742371827364\n",
            "Training epoch: 8 loss for this batch is 0.006680742371827364\n",
            "0.011931532062590122\n",
            "Training epoch: 8 loss for this batch is 0.011931532062590122\n",
            "0.013000520877540112\n",
            "Training epoch: 8 loss for this batch is 0.013000520877540112\n",
            "0.005113293882459402\n",
            "Training epoch: 8 loss for this batch is 0.005113293882459402\n",
            "0.0067581115290522575\n",
            "Training epoch: 8 loss for this batch is 0.0067581115290522575\n",
            "0.005217830650508404\n",
            "Training epoch: 8 loss for this batch is 0.005217830650508404\n",
            "0.1104678064584732\n",
            "Training epoch: 8 loss for this batch is 0.1104678064584732\n",
            "0.0031772309448570013\n",
            "Training epoch: 8 loss for this batch is 0.0031772309448570013\n",
            "0.006868652068078518\n",
            "Training epoch: 8 loss for this batch is 0.006868652068078518\n",
            "0.017974460497498512\n",
            "Training epoch: 8 loss for this batch is 0.017974460497498512\n",
            "0.009444409981369972\n",
            "Training epoch: 8 loss for this batch is 0.009444409981369972\n",
            "0.0036990719381719828\n",
            "Training epoch: 8 loss for this batch is 0.0036990719381719828\n",
            "0.0439639575779438\n",
            "Training epoch: 8 loss for this batch is 0.0439639575779438\n",
            "0.003961970563977957\n",
            "Training epoch: 8 loss for this batch is 0.003961970563977957\n",
            "0.009274170733988285\n",
            "Training epoch: 8 loss for this batch is 0.009274170733988285\n",
            "0.010307317599654198\n",
            "Training epoch: 8 loss for this batch is 0.010307317599654198\n",
            "0.004502739757299423\n",
            "Training epoch: 8 loss for this batch is 0.004502739757299423\n",
            "0.032885484397411346\n",
            "Training epoch: 8 loss for this batch is 0.032885484397411346\n",
            "0.01565414108335972\n",
            "Training epoch: 8 loss for this batch is 0.01565414108335972\n",
            "\n",
            "Test set: Average loss: 0.1677, Accuracy: 480/635 (76%)\n",
            "\n",
            "0.015110346317095192\n",
            "Epoch finished\n",
            "0.003507164539769292\n",
            "Training epoch: 9 loss for this batch is 0.003507164539769292\n",
            "0.0027778714429587126\n",
            "Training epoch: 9 loss for this batch is 0.0027778714429587126\n",
            "0.00296904263086617\n",
            "Training epoch: 9 loss for this batch is 0.00296904263086617\n",
            "0.0052346945740282536\n",
            "Training epoch: 9 loss for this batch is 0.0052346945740282536\n",
            "0.0053307306952774525\n",
            "Training epoch: 9 loss for this batch is 0.0053307306952774525\n",
            "0.00240382575429976\n",
            "Training epoch: 9 loss for this batch is 0.00240382575429976\n",
            "0.003941249568015337\n",
            "Training epoch: 9 loss for this batch is 0.003941249568015337\n",
            "0.004159773234277964\n",
            "Training epoch: 9 loss for this batch is 0.004159773234277964\n",
            "0.0037381306756287813\n",
            "Training epoch: 9 loss for this batch is 0.0037381306756287813\n",
            "0.002330922521650791\n",
            "Training epoch: 9 loss for this batch is 0.002330922521650791\n",
            "0.002691586734727025\n",
            "Training epoch: 9 loss for this batch is 0.002691586734727025\n",
            "0.0018872908549383283\n",
            "Training epoch: 9 loss for this batch is 0.0018872908549383283\n",
            "0.0032415189780294895\n",
            "Training epoch: 9 loss for this batch is 0.0032415189780294895\n",
            "0.0025559826754033566\n",
            "Training epoch: 9 loss for this batch is 0.0025559826754033566\n",
            "0.001821707934141159\n",
            "Training epoch: 9 loss for this batch is 0.001821707934141159\n",
            "0.0031384306494146585\n",
            "Training epoch: 9 loss for this batch is 0.0031384306494146585\n",
            "0.001980098430067301\n",
            "Training epoch: 9 loss for this batch is 0.001980098430067301\n",
            "0.0030019984114915133\n",
            "Training epoch: 9 loss for this batch is 0.0030019984114915133\n",
            "0.0033038107212632895\n",
            "Training epoch: 9 loss for this batch is 0.0033038107212632895\n",
            "0.0037238849326968193\n",
            "Training epoch: 9 loss for this batch is 0.0037238849326968193\n",
            "0.0011216005077585578\n",
            "Training epoch: 9 loss for this batch is 0.0011216005077585578\n",
            "0.0786961019039154\n",
            "Training epoch: 9 loss for this batch is 0.0786961019039154\n",
            "0.003283065976575017\n",
            "Training epoch: 9 loss for this batch is 0.003283065976575017\n",
            "0.003064195392653346\n",
            "Training epoch: 9 loss for this batch is 0.003064195392653346\n",
            "0.0038355085998773575\n",
            "Training epoch: 9 loss for this batch is 0.0038355085998773575\n",
            "0.004527859855443239\n",
            "Training epoch: 9 loss for this batch is 0.004527859855443239\n",
            "0.001083911512978375\n",
            "Training epoch: 9 loss for this batch is 0.001083911512978375\n",
            "0.022921305149793625\n",
            "Training epoch: 9 loss for this batch is 0.022921305149793625\n",
            "0.0023013746831566095\n",
            "Training epoch: 9 loss for this batch is 0.0023013746831566095\n",
            "0.008114428259432316\n",
            "Training epoch: 9 loss for this batch is 0.008114428259432316\n",
            "0.006175871007144451\n",
            "Training epoch: 9 loss for this batch is 0.006175871007144451\n",
            "0.002961579244583845\n",
            "Training epoch: 9 loss for this batch is 0.002961579244583845\n",
            "0.0034698189701884985\n",
            "Training epoch: 9 loss for this batch is 0.0034698189701884985\n",
            "0.05401686578989029\n",
            "Training epoch: 9 loss for this batch is 0.05401686578989029\n",
            "0.00218765065073967\n",
            "Training epoch: 9 loss for this batch is 0.00218765065073967\n",
            "0.007625970058143139\n",
            "Training epoch: 9 loss for this batch is 0.007625970058143139\n",
            "0.002883726032450795\n",
            "Training epoch: 9 loss for this batch is 0.002883726032450795\n",
            "0.005772142205387354\n",
            "Training epoch: 9 loss for this batch is 0.005772142205387354\n",
            "\n",
            "Test set: Average loss: 0.1780, Accuracy: 482/635 (76%)\n",
            "\n",
            "0.007310070835764667\n",
            "Epoch finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EbeYk5ScmUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_accuracy(loader, model):\n",
        "    # if loader.dataset.train:\n",
        "    #     print(\"Checking accuracy on training data\")\n",
        "    # else:\n",
        "    #     print(\"Checking accuracy on test data\")\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.squeeze(1)\n",
        "            y = y\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(\n",
        "            f\"Got {num_correct} / {num_samples} with accuracy  \\\n",
        "              {float(num_correct)/float(num_samples)*100:.2f}\"\n",
        "        )\n",
        "    model.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hniJ5yFYQx02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "77649508-1d8a-4d1d-ef47-9ea575f7e1fb"
      },
      "source": [
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 2360 / 2423 with accuracy                97.40\n",
            "Got 453 / 635 with accuracy                71.34\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}